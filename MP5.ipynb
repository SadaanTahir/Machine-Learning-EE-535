{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MP5: Embeddings and Sentence Classification \n",
    "\n",
    "### Introduction\n",
    "\n",
    "Embeddings are a way to represent words (or more generally, *tokens*) as vectors. These vectors are useful for many tasks in Natural Language Processing, short for NLP, including but not limited to: Text Generation, Machine Translation, and Sentence Classification. In this notebook, I will be exploring the concept of Embeddings, and using them for Sentence Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imporing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GNG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\GNG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re  # For text preprocessing\n",
    "import numpy as np  # For numerical operations and handling arrays\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import nltk  # The Natural Language Toolkit (nltk) library, useful for text processing and NLP tasks\n",
    "from nltk.corpus import stopwords  # A list of common words to filter out in text preprocessing\n",
    "from sklearn.model_selection import train_test_split  # To split datasets into training and test sets\n",
    "from sklearn.linear_model import LogisticRegression # The logistic regression model\n",
    "from sklearn.metrics import accuracy_score, classification_report # Infometrics for the created model\n",
    "import gensim.downloader as api  # To load pre-trained word embeddings\n",
    "from gensim.models.word2vec import Word2Vec  # Word2Vec model from gensim for creating and training word embeddings\n",
    "from gpt4all import Embed4All  # For generating embeddings for text\n",
    "import torch  # Deep learning library with in-built mathematical operations\n",
    "import torch.nn as nn  # For building and training models\n",
    "\n",
    "nltk.download('stopwords')  # Downloading the 'stopwords' dataset from nltk, necessary for filtering out common words in text\n",
    "nltk.download('wordnet')  # Downloading the 'wordnet' dataset, useful for lemmatization and other NLP tasks\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Embeddings\n",
    "\n",
    "Put simply, Embeddings are fixed-size **dense** vector representations of tokens in natural language. This means we can represent words as vectors, sentences as vectors, even other entities like entire graphs as vectors.\n",
    "\n",
    "So what really makes them different from something like One-Hot vectors? What's special is that they have semantic meaning baked into them. This means you can model relationships between entities in text, which itself leads to a lot of fun applications. All modern architectures make use of Embeddings in some way.\n",
    "\n",
    "More info about them can be found [here](https://aman.ai/primers/ai/word-vectors/). In this notebook, I will be using *pretrained* Embeddings, that have already been trained on a large corpus of text. This is primarily because training Embeddings from scratch is a very computationally expensive task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading word2vec model!\n"
     ]
    }
   ],
   "source": [
    "# Downloading the pretrained word2vec model (this may take a few minutes)\n",
    "corpus = api.load('text8') # text8 is a small corpus of compressed Wikipedia articles\n",
    "w2vmodel = Word2Vec(corpus) # The w2vmodel learns vector representations of words from the downlaoded text corpus\n",
    "\n",
    "print(\"Done loading word2vec model!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Embeddings have been loaded, we can create an Embedding **layer** in PyTorch, `nn.Embedding`, that will perform the processing step for us.\n",
    "\n",
    "Note in the following cell how there is a given **vocab size** and **embedding dimension** for the model: this is important to note because some sets of Embeddings may be defined for a large set of words (a large vocab), whereas older ones perhaps have a smaller set (a small vocab); the Embedding dimension essentially tells us how many *features* have been learned for a given word, that will allow us to perform further processing on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 71290\n",
      "Some of the words in the vocabulary:\n",
      "['the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero', 'nine', 'two', 'is', 'as', 'eight', 'for', 's']\n",
      "Embedding dimension: 100\n"
     ]
    }
   ],
   "source": [
    "# Defining embedding layer using gensim\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.FloatTensor(w2vmodel.wv.vectors))\n",
    "\n",
    "# Getting some information from the w2vmodel\n",
    "print(f\"Vocab size: {len(w2vmodel.wv.key_to_index)}\")\n",
    "\n",
    "print(f\"Some of the words in the vocabulary:\\n{list(w2vmodel.wv.key_to_index.keys())[:15]}\")\n",
    "\n",
    "print(f\"Embedding dimension: {w2vmodel.wv.vectors.shape[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for a demonstration, we instantiate two words, turn them into numbers (encoding them via their index in the vocab), and pass them through the Embedding layer. \n",
    "\n",
    "Note how the resultant Embeddings both have the same shape: 1 word, and 100 elements in the vector representing that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Shape for 'king': torch.Size([1, 100])\n",
      "Embedding Shape for 'queen': torch.Size([1, 100])\n"
     ]
    }
   ],
   "source": [
    "# Taking two words and getting their embeddings\n",
    "word1 = \"king\"\n",
    "word2 = \"queen\"\n",
    "\n",
    "def word2vec(word):\n",
    "    return embedding_layer(torch.LongTensor([w2vmodel.wv.key_to_index[word]]))\n",
    "\n",
    "king_embedding = word2vec(word1)\n",
    "queen_embedding = word2vec(word2)\n",
    "\n",
    "print(f\"Embedding Shape for '{word1}': {king_embedding.shape}\")\n",
    "print(f\"Embedding Shape for '{word2}': {queen_embedding.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have vectors whose scale is arbitrary, one nice way to measure how *similar* they are is with the Cosine Similarity measure.\n",
    "\n",
    "\n",
    "$$ \\text{Cosine Similarity}(\\mathbf{u},\\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|} $$\n",
    "\n",
    "\n",
    "We can apply this idea to our Embeddings. To see how \"similar\" two words are to the model, we can generate their Embeddings and take the Cosine Similarity of those embeddings. This will be a number between -1 and 1 (just like the range of the cosine function). When the number is close to 0, the words are not similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'king' and 'queen': 0.7342362403869629\n",
      "Similarity between 'king' and 'earth': 0.012497778050601482\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    '''\n",
    "    Computes the cosine similarity between two vectors using (PyTorch)\n",
    "    '''\n",
    "    \n",
    "    cosine_simi = torch.dot(vec1, vec2) / (torch.norm(vec1) * torch.norm(vec2))\n",
    "    return cosine_simi.item()\n",
    "\n",
    "def compute_word_similarity(word1, word2):\n",
    "    '''\n",
    "    Takes in two words, computes their embeddings and returns the cosine similarity\n",
    "    '''\n",
    "    # without using .view(-1), I get dimentionality issues within the tensors\n",
    "    return cosine_similarity(word2vec(word1).view(-1), word2vec(word2).view(-1))\n",
    "\n",
    "# Defining three words (one pair similar and one pair dissimilar) and computing their similarity\n",
    "word1 = \"king\"\n",
    "word2 = \"queen\"\n",
    "word3 = \"earth\"\n",
    "print(f\"Similarity between '{word1}' and '{word2}': {compute_word_similarity(word1, word2)}\")\n",
    "print(f\"Similarity between '{word1}' and '{word3}': {compute_word_similarity(word1, word3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you're done with the above section\n",
    "del embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Classification with Sentence Embeddings\n",
    "\n",
    "Now let's move on to an actual application: classifying whether a tweet is about a real disaster or not. As you can imagine, this could be a valuable model when monitoring social media for disaster relief efforts.\n",
    "\n",
    "Since we are using Sentence Embeddings, we want something that will take in a sequence of words and throw out a single fixed-size vector. For this task, we will make use of an LLM via the `gpt4all` library.\n",
    "\n",
    "This library will allow us to generate pretrained embeddings for sentences, that we can use as **features** to feed to any classifier of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 2)\n",
      " \n",
      "Printing a few rows of the training data:\n",
      "                                                   text  target\n",
      "4557  #golf McIlroy fuels PGA speculation after vide...       0\n",
      "6443  @RayquazaErk There are Christian terrorists to...       1\n",
      "1397  Warfighting Robots Could Reduce Civilian Casua...       1\n",
      "592   FedEx no longer to transport bioterror germs i...       1\n",
      "5871  You can only make yourself happy. Fuck those t...       0\n",
      " \n",
      "Printing a few rows of the validation data:\n",
      "                                                   text  target\n",
      "1998  'Mages of Fairy Tail.. Specialize in property ...       0\n",
      "7116  Storm blitzes Traverse City disrupts Managemen...       1\n",
      "7060  Series finale of #TheGame :( It survived so mu...       0\n",
      "4908  @nataliealund \\nParents of Colorado theater sh...       1\n",
      "5510  Reddit's new content policy goes into effect m...       0\n",
      " \n",
      "Training data: (6851, 2) Validation data: (762, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reading in the data over here\n",
    "df = pd.read_csv(\"./disaster_tweets.csv\")\n",
    "df = df[[\"text\", \"target\"]]\n",
    "print(df.shape)\n",
    "print(\" \")\n",
    "\n",
    "# Splitting the data into train and test\n",
    "...\n",
    "train, val = train_test_split(df, test_size = 0.1, random_state = 420, stratify = df['target'])\n",
    "print(\"Printing a few rows of the training data:\")\n",
    "print(train.head())\n",
    "\n",
    "print(\" \")\n",
    "print(\"Printing a few rows of the validation data:\")\n",
    "print(val.head())\n",
    "\n",
    "print(\" \")\n",
    "print(\"Training data:\", train.shape, \"Validation data:\", val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping straight to Embeddings, since our data is sourced from the cesspool that is Twitter (now X), we should probably do some cleaning. This can involve the removal of URLs, punctuation, numbers that don't provide any meaning, stopwords, and so on.\n",
    "\n",
    "In the following cell, I have written functions to clean the sentences. \n",
    "\n",
    "**Note:** After cleaning the sentences, it is possible that we may end up with empty sentences (or some that are so short they have lost all meaning). In this event, since we want to demonstrate setting up a Sentence Classification task, I removed them from the dataset (cuz like data cleaning is not the center of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "                                                   text  target  \\\n",
      "4557  #golf McIlroy fuels PGA speculation after vide...       0   \n",
      "6443  @RayquazaErk There are Christian terrorists to...       1   \n",
      "1397  Warfighting Robots Could Reduce Civilian Casua...       1   \n",
      "592   FedEx no longer to transport bioterror germs i...       1   \n",
      "5871  You can only make yourself happy. Fuck those t...       0   \n",
      "\n",
      "                                           cleaned_text  \n",
      "4557  golf mcilroy fuels pga speculation video injur...  \n",
      "6443  rayquazaerk christian terrorists sure dont sui...  \n",
      "1397  warfighting robots could reduce civilian casua...  \n",
      "592   fedex longer transport bioterror germs wake an...  \n",
      "5871            make happy fuck tryna ruin keep smiling  \n",
      "(6613, 3)\n",
      " \n",
      "Validation Data\n",
      "                                                   text  target  \\\n",
      "1998  'Mages of Fairy Tail.. Specialize in property ...       0   \n",
      "7116  Storm blitzes Traverse City disrupts Managemen...       1   \n",
      "7060  Series finale of #TheGame :( It survived so mu...       0   \n",
      "4908  @nataliealund \\nParents of Colorado theater sh...       1   \n",
      "5510  Reddit's new content policy goes into effect m...       0   \n",
      "\n",
      "                                           cleaned_text  \n",
      "1998  mages fairy tail specialize property damage na...  \n",
      "7116  storm blitzes traverse city disrupts managemen...  \n",
      "7060  series finale thegame survived much upheaval a...  \n",
      "4908  nataliealund parents colorado theater shooting...  \n",
      "5510  reddits new content policy goes effect many ho...  \n",
      "(742, 3)\n"
     ]
    }
   ],
   "source": [
    "# Functions for cleaning the data\n",
    "def lowercase(txt):\n",
    "    return txt.lower()\n",
    "\n",
    "def remove_punctuation(txt):\n",
    "    return re.sub(r'[^\\w\\s]', '', txt)\n",
    "\n",
    "def remove_stopwords(txt):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = txt.split()\n",
    "    filtered_words = [word for word in words if lowercase(word) not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def remove_numbers(txt):\n",
    "    return re.sub(r'\\d', '', txt)\n",
    "\n",
    "def remove_url(txt):\n",
    "    return re.sub(r'http\\S+', '', txt)\n",
    "\n",
    "def normalize_sentence(txt):\n",
    "    '''\n",
    "    Aggregates all the above functions to normalize/clean a sentence\n",
    "    '''\n",
    "    txt = lowercase(txt)\n",
    "    txt = remove_punctuation(txt)\n",
    "    txt = remove_stopwords(txt)\n",
    "    txt = remove_numbers(txt)\n",
    "    txt = remove_url(txt)\n",
    "    return txt\n",
    "\n",
    "# Cleaning the sentences\n",
    "train.loc[:, 'cleaned_text'] = train['text'].apply(normalize_sentence)\n",
    "val.loc[:, 'cleaned_text'] = val['text'].apply(normalize_sentence)\n",
    "\n",
    "# Filtering sentences that are too short (less than 20ish characters)\n",
    "min_characters = 20\n",
    "train = train[train['cleaned_text'].str.len() >= min_characters]\n",
    "val = val[val['cleaned_text'].str.len() >= min_characters]\n",
    "\n",
    "# Printing the now clean training and validation data\n",
    "print(\"Train Data\")\n",
    "print(train.head())\n",
    "print(train.shape)\n",
    "print(\" \")\n",
    "print(\"Validation Data\")\n",
    "print(val.head())\n",
    "print(val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the Embeddings!\n",
    "\n",
    "We will be using the `gpt4all.Embed4All` class for this purpose. The documentation can be looked up over [here](https://docs.gpt4all.io/gpt4all_python_embedding.html#gpt4all.gpt4all.Embed4All.embed).\n",
    "\n",
    "This functionality makes use of a model called [Sentence-BERT](https://arxiv.org/abs/1908.10084). This is a Transformer-based model that has been trained on a large corpus of text, and is able to generate high-quality Sentence Embeddings, exactly what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 45.9M/45.9M [00:04<00:00, 9.43MiB/s]\n",
      "Verifying: 100%|██████████| 45.9M/45.9M [00:00<00:00, 768MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# Generating embeddings for train and validation sentences\n",
    "feature_extractor = Embed4All()\n",
    "\n",
    "# Encoding the train samples\n",
    "train_samples = train['cleaned_text'].tolist()\n",
    "train_embeddings = [feature_extractor.embed(sentence) for sentence in train_samples]\n",
    "\n",
    "# Encoding the validation sentences\n",
    "validation_samples = val['cleaned_text'].tolist()\n",
    "validation_embeddings = [feature_extractor.embed(sentence) for sentence in validation_samples]\n",
    "\n",
    "# Preparing the labels\n",
    "train_labels = train['target'].tolist()\n",
    "val_labels = val['target'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6613 , 6613\n",
      "\n",
      "742 , 742\n"
     ]
    }
   ],
   "source": [
    "# Printing the lengths of the embeddings and their labels\n",
    "print(len(train_labels), \",\",len(train_embeddings))\n",
    "print(\"\")\n",
    "print(len(val_labels), \",\",len(validation_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with our Embeddings ready, we can move on to the actual classification task.\n",
    "\n",
    "You have the choice of using **any** classifier you wish. You can use a simple Logistic Regression model, get fancy with Support Vector Machines, or even use a Neural Network. The choice is yours.\n",
    "\n",
    "We will be looking for a model with a **Validation Accuracy** of around $0.8$. You must also use this model to make predictions on your own provided inputs, after completing the `predict` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy [%]:  82.0754716981132\n"
     ]
    }
   ],
   "source": [
    "# Converting lists to NumPy arrays\n",
    "x_train = train_embeddings\n",
    "y_train = train_labels\n",
    "x_val = validation_embeddings\n",
    "y_val = val_labels\n",
    "\n",
    "model = LogisticRegression(random_state=420)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_val_predicted = model.predict(x_val)\n",
    "\n",
    "val_acc = accuracy_score(y_val, y_val_predicted)\n",
    "print(\"Validation Accuracy [%]: \", val_acc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to predict on a sentence\n",
    "def predict(sentence, clf):\n",
    "    '''\n",
    "    Takes in a sentence and returns the predicted class along with the probability\n",
    "    '''\n",
    "    # Cleaning and encoding the sentence\n",
    "    cleaned_sentence = normalize_sentence(sentence)\n",
    "    sentence_embedding = feature_extractor.embed(cleaned_sentence)\n",
    "\n",
    "    # Predicting the class and probability\n",
    "    prediction = clf.predict([sentence_embedding])[0]\n",
    "    probability = clf.predict_proba([sentence_embedding])[0][1]\n",
    "\n",
    "    return prediction, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: My life is nothing short of a disaster at the moment \n",
      "Prediction: 1\n",
      "Probability: 0.62\n",
      "\n",
      "Sentence: This semester was not a disaster surprisingly\n",
      "Prediction: 1\n",
      "Probability: 0.54\n",
      "\n",
      "Sentence: Peaceful disaster\n",
      "Prediction: 1\n",
      "Probability: 0.90\n",
      "\n",
      "Sentence: Disaster strikes in the city.\n",
      "Prediction: 1\n",
      "Probability: 0.95\n",
      "\n",
      "Sentence: Disaster did not strike in the country\n",
      "Prediction: 1\n",
      "Probability: 0.94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on a few custom sentences\n",
    "sentences_to_predict = [\n",
    "    \"My life is nothing short of a disaster at the moment \",\n",
    "    \"This semester was not a disaster surprisingly\",\n",
    "    \"Peaceful disaster\",\n",
    "    \"Disaster strikes in the city.\",\n",
    "    \"Disaster did not strike in the country\",\n",
    "]\n",
    "\n",
    "# Predict on the sentences\n",
    "for sentence in sentences_to_predict:\n",
    "    prediction, probability = predict(sentence, model)\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"Probability: {probability:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbe381b710e5d3541ca1e32a0f143d44d9fc319722adcf51c48d4250c2e9fef8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
